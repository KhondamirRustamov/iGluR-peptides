{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2d8004-8040-4688-a9e5-d82f342b07ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import re\n",
    "from string import ascii_uppercase, ascii_lowercase\n",
    "from math import sqrt\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from colabfold.download import download_alphafold_params, default_data_dir\n",
    "from colabfold.utils import setup_logging\n",
    "from colabfold.batch import get_queries, run, set_model_type\n",
    "\n",
    "from colabfold.colabfold import plot_protein\n",
    "from pathlib import Path\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "    \n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aae13345-c93f-41e8-b800-b53b658df04d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 3), stride=(1, 2), padding=(0, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 3), stride=(1, 2), padding=(0, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 3), stride=(1, 2), padding=(0, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 1\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 1000\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0001\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 5, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 5 x 5\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 5, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 10 x 10\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, (4,3), (1,2), (0,1), bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 10 x 20\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, (4,3), (1,2), (0,1), bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 20 x 40\n",
    "            nn.ConvTranspose2d( ngf, nc, (4,3), (1,2), (0,1), bias=False),\n",
    "            # state size. (nc) x 20 x 80\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.02.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)\n",
    "\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "074c02f1-5f5c-4139-a599-17753a96932a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def alphafold_predict(sequence, epoch, item):\n",
    "  result_dir=\"./results/\"\n",
    "  query_sequence = \"\".join(sequence.split())\n",
    "\n",
    "  jobname = 'tmp'+'_'+str(epoch)+'_'+str(item)\n",
    "\n",
    "  with open(f\"{result_dir}{jobname}.csv\", \"w\") as text_file:\n",
    "      text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n",
    "\n",
    "  queries_path=f\"{result_dir}{jobname}.csv\"\n",
    "\n",
    "  # number of models to use\n",
    "  use_amber = False \n",
    "  template_mode = \"custom\" \n",
    "  \n",
    "\n",
    "  custom_template_path = 'custom_template'\n",
    "  use_templates = True\n",
    "\n",
    "  msa_mode = \"single_sequence\" \n",
    "  pair_mode = \"unpaired+paired\" \n",
    "  \n",
    "\n",
    "  # decide which a3m to use\n",
    "  if msa_mode.startswith(\"MMseqs2\"):\n",
    "    a3m_file = f\"{jobname}.a3m\"\n",
    "  model_type = \"auto\"\n",
    "  num_recycles = 3 \n",
    "  save_to_google_drive = False \n",
    "\n",
    "  dpi = 200\n",
    "    \n",
    "  def prediction_callback(unrelaxed_protein, length, prediction_result, input_features, type):\n",
    "    fig = plot_protein(unrelaxed_protein, Ls=length, dpi=150)\n",
    "\n",
    "  \n",
    "  if 'logging_setup' not in globals():\n",
    "      setup_logging(Path(\".\").joinpath(\"log.txt\"))\n",
    "      logging_setup = True\n",
    "\n",
    "  queries, is_complex = get_queries(queries_path)\n",
    "  model_type = set_model_type(is_complex, model_type)\n",
    "  download_alphafold_params(model_type, Path(\".\"))\n",
    "  run(\n",
    "      queries=queries,\n",
    "      result_dir=result_dir,\n",
    "      use_templates=use_templates,\n",
    "      custom_template_path=custom_template_path,\n",
    "      use_amber=use_amber,\n",
    "      msa_mode=msa_mode,    \n",
    "      model_type=model_type,\n",
    "      num_models=1,\n",
    "      num_recycles=num_recycles,\n",
    "      model_order=[1],\n",
    "      is_complex=is_complex,\n",
    "      data_dir=Path(\".\"),\n",
    "      keep_existing_results=False,\n",
    "      recompile_padding=1.0,\n",
    "      rank_by=\"auto\",\n",
    "      pair_mode=pair_mode,\n",
    "      stop_at_score=float(100),\n",
    "      #prediction_callback=prediction_callback,\n",
    "      dpi=dpi\n",
    "  )\n",
    "  for i in glob.glob(f'results/{jobname}*'):\n",
    "    if not os.path.isfile(i):\n",
    "        os.system(f'rm -r {i}')\n",
    "    else:\n",
    "        if 'pdb' not in i and 'scores_' not in i:\n",
    "            os.remove(i)\n",
    "  for i in glob.glob('custom_template/*'):\n",
    "    if '.pdb' not in i:\n",
    "        os.remove(i)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "aa = 'A R N D C E Q G H I L K M F P S T W Y V'\n",
    "aminoacid_list = aa.split(' ')\n",
    "a_dict={}\n",
    "for i, aaa in enumerate(aminoacid_list):\n",
    "  a_dict[aaa]=i\n",
    "\n",
    "\n",
    "def numpy_to_seq(array, seq_len, oligomer=1):\n",
    "  sequence = []\n",
    "  receptor = 'GSNKTVVVTTILESPYVMMKKNHEMLEGNERYEGYCVDLAAEIAKHCGFKYKLTIVGDGKYGARDADTKIWNGMVGELVYGKADIAIAPLTITLVREEVIDFSKPFMSLGISIMIKKGTPIESAEDLSKQTEIAYGTLDSGSTKEFFRRSKIAVFDKMWTYMRSAEPSVFVRTTAEGVARVRKSKGKYAYLLESTMNEYIEQRKPCDTMKVGGNLDSKGYGIATPKGSSLRNAVNLAVLKLNEQGLLDKLKNKWWYDKGECGS'\n",
    "  array = array.cpu().detach().numpy()\n",
    "  for i in array:\n",
    "    seq=''\n",
    "    array_n = i[0]\n",
    "    for i in array_n.T:\n",
    "      z = np.argmax(i)\n",
    "      seq+=aminoacid_list[z]\n",
    "    seq = seq[:seq_len]\n",
    "    seq = receptor + ':' + seq\n",
    "    sequence.append(seq)\n",
    "  return sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e14375d-527c-4c4e-a26d-4f1dff10447e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "\n",
    "import Bio\n",
    "import Bio.PDB\n",
    "import Bio.SeqRecord\n",
    "\n",
    "\n",
    "def read_pdb(pdbcode, pdbfilenm):\n",
    "    \"\"\"\n",
    "    Read a PDB structure from a file.\n",
    "    :param pdbcode: A PDB ID string\n",
    "    :param pdbfilenm: The PDB file\n",
    "    :return: a Bio.PDB.Structure object or None if something went wrong\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdbparser = Bio.PDB.PDBParser(QUIET=True)   # suppress PDBConstructionWarning\n",
    "        struct = pdbparser.get_structure(pdbcode, pdbfilenm)\n",
    "        return struct\n",
    "    except Exception as err:\n",
    "        print(str(err), file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "\n",
    "def define_distance_matrices(epoch):\n",
    "    #selections_a = [['A', '46'], ['A', '237'], ['A', '241'], ['A', '245'], ['A', '248'], ['A', '249']]\n",
    "    #numbers_a = [int(x[1]) for x in selections_a]\n",
    "    numbers_a = [5,70,148,152,164,166,168,169,172,173,260,262,263,267]\n",
    "    #numbers_a = [int(x) for x in numbers_a]\n",
    "    \n",
    "    selections_b = [['B', '93'], ['B', '94'], ['B', '97'], ['B', '151'], ['B', '153']]\n",
    "    numbers_b = [int(x[1]) for x in selections_b]\n",
    "    \n",
    "    mins_list = []\n",
    "    \n",
    "    for i in glob.glob(f'results/tmp_{epoch}_*.pdb'):\n",
    "        pdb_5f1c = read_pdb('5f1c', i)\n",
    "    \n",
    "        ChainA = [Chain for Chain in pdb_5f1c.get_chains() if Chain.__repr__() == '<Chain id=A>'][0]\n",
    "        calphas_A = [atom for atom in ChainA.get_atoms() if \"CA\" in atom.get_name()]\n",
    "        calphas_A_coord = [atom.get_coord() for atom in calphas_A if atom.get_parent().get_full_id()[3][1] in numbers_a]\n",
    "        \n",
    "        #ChainB = [ Chain for Chain in pdb_5f1c.get_chains() if Chain.__repr__() == '<Chain id=B>'][0]\n",
    "        #calphas_B = [atom for atom in ChainB.get_atoms() if \"CA\" in atom.get_name()]\n",
    "        #calphas_B_coord = [atom.get_coord() for atom in calphas_B if atom.get_parent().get_full_id()[3][1] in numbers_b]\n",
    "        \n",
    "        target = calphas_A_coord #+ calphas_B_coord\n",
    "        \n",
    "        center = sum(target)/len(target)\n",
    "        \n",
    "        ChainC = [ Chain for Chain in pdb_5f1c.get_chains() if Chain.__repr__() == '<Chain id=B>'][0]\n",
    "        calphas_C_coord = [atom.get_coord() for atom in ChainC.get_atoms() if \"CA\" in atom.get_name()]\n",
    "        \n",
    "        dist = distance_matrix(target, calphas_C_coord)\n",
    "        mins = np.array([np.min(x) for x in dist.T])\n",
    "        \n",
    "        mins_list.append(mins)\n",
    "    \n",
    "    mins_list = np.array(mins_list)\n",
    "    target_list = np.ones(mins_list.shape)\n",
    "    \n",
    "    return torch.tensor(target_list).float(), torch.tensor(mins_list).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "254962f2-d839-48f1-bf83-09d6503407a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def take_loss(target_pdb_name, epoch, seq_len):\n",
    "  all_loss = [take_loss1(i) for i in glob.glob(f'results/tmp_{epoch}_*.pdb')]\n",
    "  true_tensor = [0 for i in range(seq_len)]\n",
    "  return torch.tensor(true_tensor).float(), torch.tensor(all_loss).float()\n",
    "\n",
    "\n",
    "def take_loss1(filename):\n",
    "  with open(filename) as ifile:\n",
    "      system = \"\".join([x for x in ifile])\n",
    "  system1 = system.split(\"\\n\")\n",
    "  system2 = []\n",
    "  for x in system1:\n",
    "    if x[:4] == 'ATOM':\n",
    "      system2.append(x)\n",
    "  CAS = [x for x in system2 if \"CA\" in x]\n",
    "  CAS = [x.split(' ') for x in CAS]\n",
    "  CAS = [[x for x in y if x!=''] for y in CAS ]\n",
    "  CAS_A = [x for x in CAS if x[4]=='B']\n",
    "  CAS_B = [x for x in CAS if x[4]=='A']\n",
    "  numbers_a = [13,14,174,175,178,179,182,196,199,200,203,205,260]\n",
    "\n",
    "  CAS_A = np.array([np.array([float(x[6]), float(x[7]), float(x[8])]) for x in CAS_A])\n",
    "\n",
    "  CAS_B = np.array([np.array([float(x[6]), float(x[7]), float(x[8])]) for x in CAS_B])\n",
    "\n",
    "  CAS_B = np.array([CAS_B[i-1] for i in numbers_a])\n",
    "\n",
    "  receptor_center = (sum([i for i in CAS_B]))/CAS_B.shape[0]\n",
    "  peptide_center = (sum([i for i in CAS_A]))/CAS_A.shape[0]\n",
    "  loss = sqrt(np.sum((peptide_center-receptor_center)**2))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1737435-7ce4-4eca-80d5-cb52b07bdf86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def training(num_epochs=10, num_seqs=20, peptide_length=20):\n",
    "    # Training Loop\n",
    "    sequences = []\n",
    "    G_losses = []\n",
    "    predicted_losses_list = []\n",
    "    epoch_nums=[]\n",
    "    # tables=[]\n",
    "\n",
    "    print(\"Starting Training Loop...\")\n",
    "    # For each epoch\n",
    "    netG.apply(weights_init)\n",
    "    for epoch in range(num_epochs):\n",
    "        if True:\n",
    "            start = time.time()\n",
    "            # For each batch in the dataloader\n",
    "            fake = netG(torch.randn(num_seqs, nz, 1, 1, device=device))\n",
    "            fake = numpy_to_seq(fake, peptide_length)\n",
    "            sequences+=[x.split(':')[-1] for x in fake]\n",
    "\n",
    "            for z, x in enumerate(fake):\n",
    "              alphafold_predict(x, epoch, z)\n",
    "            print('time:', time.time() - start)\n",
    "            optimizerG.zero_grad()\n",
    "\n",
    "            # Calculate G's loss based on this output\n",
    "            target, pred_loss = take_loss('results/tmp', epoch, peptide_length)\n",
    "            errG = criterion(target, pred_loss).requires_grad_(True)\n",
    "            print(errG)\n",
    "            # Calculate gradients for G\n",
    "            errG.backward()\n",
    "            # Update G\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Output training stats\n",
    "            print(f'{epoch + 1}/{num_epochs}, {errG.item()}')\n",
    "            # tables.append(fake.detach().numpy()[0][0])\n",
    "            epoch_nums+=[f'{epoch}_{z}' for z, x in enumerate(fake)]\n",
    "            # Save Losses for plotting later\n",
    "            G_losses.append(errG.item())\n",
    "            predicted_losses_list+=[torch.mean(x).item() for x in pred_loss]\n",
    "            df = {'id': epoch_nums,\n",
    "                  'sequence': sequences,\n",
    "                  'predicted_loss': predicted_losses_list}\n",
    "            df = pd.DataFrame(df)\n",
    "            df.to_csv('Khondamir_results.csv')\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a733f9-1dfe-4fc5-a826-e0ffb63c325d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training(50, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0a683-2175-4662-a638-25d72d807423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
